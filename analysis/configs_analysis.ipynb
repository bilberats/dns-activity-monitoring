{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "455e3bef",
   "metadata": {},
   "source": [
    "# Analysis DNS consumption with multiple configurations\n",
    "\n",
    "Configurations have different QPS and protocols used.\n",
    "This notebook aims to visualize the difference of consumption between those configurations.\n",
    "Later we will use models to predict the agregated consumption of those different configurations.\n",
    "In those configurations, the cache is *always used* because the domain is always the same in the requests (example.com).\n",
    "\n",
    "## What will visualisations will be shown\n",
    "\n",
    "The different things we want to study are:\n",
    "- First an analysis on queries:\n",
    "    - Idle consumption\n",
    "    - QPS for different protocols received/sent by DNS and PC\n",
    "    - The agregated consumption in function of QPS (Query Per Second)\n",
    "    - The agregated consumption in function of the protocol \n",
    "- Then analysis on PC metrics:\n",
    "    - Visualisation of the metrics for different protocols and different QPS\n",
    "    - Correlation matrix of all metrics with protocols and QPS in the metrics\n",
    "- Visualisation of the predicted aggregated consumption made by different models\n",
    "\n",
    "## The model training part\n",
    "\n",
    "Different models will be train to predict the aggregated consumption of the DNS server (Random forest, XGBoost, Linear regression and LSTM).\n",
    "The error will be measured using RMSE (Root Mean Square Error).\n",
    "A MAPE (Mean Absolute Percentage Error) will give us the error % for all models trained.\n",
    "\n",
    "## Data transformations\n",
    "\n",
    "Different type of files are used for this analysis (.log and .csv). They will all be imported using Pandas creating DataFrames.\n",
    "Here are all the files used:\n",
    "- CSV files:\n",
    "    - `cpu_power.csv` : Metrics for the cpu consumption\n",
    "    - `io_power.csv` : Metrics for the I/O (Disk)\n",
    "    - `nic_power.csv` : Metrics for the NIC (Network Interface Card)\n",
    "    - `ram_power.csv` : Metrics fo the ram utilisation (only LLC-loads and LLC-store, Last Level Cache not really RAM but we can estimate ram consumption from it)\n",
    "    - `yoctowatt.csv` : External measurements made with a YoctoPuce\n",
    "- Log files:\n",
    "    - `bind_queries_captired.log` : Queries captured by the Bind DNS\n",
    "    - `dns_doh.log` : DoH queries captured by the server\n",
    "    - `dns_tcp.log` : DoT queries captured by the server\n",
    "    - `dns_udp.log` : UDP queries captured by the server\n",
    "\n",
    "Log files are used for their queries received at the datetime given.\n",
    "CSV files contain TimeSeries metrics.\n",
    "\n",
    "All DataFrames are linked with Date time.\n",
    "\n",
    "#### NaN gestion\n",
    "\n",
    "Because all measures don't have the same frequency (100Hz for the YoctoPuce and 1Hz for the rest) and are not exactly synchronised, NaN values will be created when merging them.\n",
    "NaN values will be interpolated for all measures metrics with `bfill` and NaN from logs will be set to 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66be5874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8861436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGURATIONS = {\n",
    "    \"QPS\": [1, 10, 100, 500, 1000],\n",
    "    \"Protocols\": [\"udp\", \"dot\", \"doh\"]\n",
    "}\n",
    "\n",
    "DURATION = 300 # seconds\n",
    "ROOT = \"../data/300_config_merged/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f18bcad",
   "metadata": {},
   "source": [
    "## We need to collect all files from all those configurations\n",
    "\n",
    "A function will help collect all files from one configuration.  \n",
    "All configurations are stored in folders with name `{DURATION}s_{PROTOCOL}_{QPS}qps_{TIMESTAMP}`.  \n",
    "To simplify this, folders will be collected ignoring the timestamp just with the begining of the folder name.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140fec67",
   "metadata": {},
   "source": [
    "#### Function to import and process log files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2869ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def import_log_file(file_path, offset_seconds=0, offset_date=None):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        log_df = pd.DataFrame({\"raw\": f.read().splitlines()})\n",
    "        \n",
    "    pattern = r'^(?P<date>\\d{2}-[A-Za-z]{3}-\\d{4})\\s+(?P<time>\\d{2}:\\d{2}:\\d{2}\\.\\d+).*$'\n",
    "    if offset_date:\n",
    "        pattern = r'^(?P<time>\\d{2}:\\d{2}:\\d{2}\\.\\d+).*$'\n",
    "    \n",
    "    log_df = log_df[\"raw\"].str.extract(pattern)\n",
    "    \n",
    "    if offset_date:\n",
    "        log_df[\"date\"] = offset_date.strftime(\"%d-%b-%Y\")\n",
    "\n",
    "    log_df[\"datetime\"] = pd.to_datetime(log_df[\"date\"] + \" \" + log_df[\"time\"], format=\"%d-%b-%Y %H:%M:%S.%f\")\n",
    "    log_df[\"datetime\"] = log_df[\"datetime\"] + pd.to_timedelta(-offset_seconds, unit='s')\n",
    "    log_df = log_df.drop(columns=[\"date\", \"time\"])\n",
    "\n",
    "    log_df.dropna(inplace=True)\n",
    "    if offset_date is None:\n",
    "        date_offset = log_df[\"datetime\"].iloc[0].date()\n",
    "    \n",
    "    log_df = log_df.set_index(\"datetime\").sort_index()\n",
    "    \n",
    "    if offset_date:\n",
    "        return log_df\n",
    "    return log_df,date_offset\n",
    "\n",
    "def import_all_logs(folder_path):\n",
    "    log_files = [\n",
    "        \"bind_queries_captured.log\",\n",
    "        \"dns_doh.log\",\n",
    "        \"dns_tcp.log\",\n",
    "        \"dns_udp.log\",\n",
    "    ]\n",
    "\n",
    "    dfs = []\n",
    "    for log_file in log_files:\n",
    "        file_path = os.path.join(folder_path, \"logs\", log_file)\n",
    "        if \"captured\" in log_file:\n",
    "            df, date_offset = import_log_file(file_path)\n",
    "        else:\n",
    "            df = import_log_file(file_path, offset_seconds=3600, offset_date=date_offset)\n",
    "        dfs.append(df)\n",
    "        \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e96e091",
   "metadata": {},
   "source": [
    "#### Function to import and process the 5 CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88344b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_csv_files(folder_path):\n",
    "    # Importing cpu_power.csv\n",
    "    cpu_power = pd.read_csv(f\"{folder_path}/measures/cpu_power.csv\")\n",
    "    # convert Time_Of_Day_Seconds to int\n",
    "    cpu_power[\"datetime\"] = pd.to_datetime(cpu_power[\"Time_Of_Day_Seconds\"], unit='s')\n",
    "    cpu_power.drop(columns=[\"Time_Of_Day_Seconds\"], inplace=True)\n",
    "\n",
    "    # Importing io_power.csv\n",
    "    io_power = pd.read_csv(f\"{folder_path}/measures/io_power.csv\")\n",
    "    io_power[\"datetime\"] = pd.to_datetime(io_power[\"timestamp\"], unit='s')\n",
    "    io_power.drop(columns=[\"timestamp\"], inplace=True)\n",
    "\n",
    "    # Importing nic_power.csv\n",
    "    nic_power = pd.read_csv(f\"{folder_path}/measures/nic_power.csv\")\n",
    "    nic_power[\"datetime\"] = pd.to_datetime(nic_power[\"timestamp\"], unit='s')\n",
    "    nic_power.drop(columns=[\"timestamp\"], inplace=True)\n",
    "\n",
    "    # Importing ram_power.csv\n",
    "    ram_power = pd.read_csv(f\"{folder_path}/measures/ram_power.csv\")\n",
    "    ram_power[\"datetime\"] = pd.to_datetime(ram_power[\"timestamp\"], unit='s')\n",
    "    ram_power.drop(columns=[\"timestamp\"], inplace=True)\n",
    "\n",
    "    # Importing yoctowatt.csv\n",
    "    yoctowatt = pd.read_csv(f\"{folder_path}/measures/yoctowatt.csv\")\n",
    "    yoctowatt[\"datetime\"] = pd.to_datetime(yoctowatt[\"timestamp_unix\"], unit='s')\n",
    "    yoctowatt = yoctowatt.drop(columns=[\"reportFrequency_effective\", \"timestamp_iso\", \"timestamp_unix\"])\n",
    "\n",
    "    return [cpu_power, io_power, nic_power, ram_power, yoctowatt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd2cec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_df_from_config(QPS, protocol):\n",
    "    folder_name_start = f\"{DURATION}s_{protocol}_{QPS}qps_\"\n",
    "    folder_name = next((f for f in os.listdir(ROOT) if f.startswith(folder_name_start)), None)\n",
    "    \n",
    "    logs_dfs = import_all_logs(os.path.join(ROOT, folder_name))\n",
    "    measures_dfs = import_csv_files(os.path.join(ROOT, folder_name))\n",
    "\n",
    "    return {\n",
    "        \"logs\": logs_dfs,\n",
    "        \"measures\": measures_dfs,\n",
    "        \"QPS\": QPS,\n",
    "        \"Protocol\": protocol\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2be23d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_idle_df():\n",
    "    folder_name_start = f\"idle_\"\n",
    "    folder_name = next((f for f in os.listdir(ROOT) if f.startswith(folder_name_start)), None)\n",
    "    \n",
    "    logs_dfs = import_all_logs(os.path.join(ROOT, folder_name))\n",
    "    measures_dfs = import_csv_files(os.path.join(ROOT, folder_name))\n",
    "\n",
    "    return {\n",
    "        \"logs\": logs_dfs,\n",
    "        \"measures\": measures_dfs,\n",
    "        \"QPS\": 0,\n",
    "        \"Protocol\": \"idle\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d0d877",
   "metadata": {},
   "source": [
    "#### Collect all DataFrames from all configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b065dce",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m qps \u001b[38;5;129;01min\u001b[39;00m CONFIGURATIONS[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQPS\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m      4\u001b[0m         configs\u001b[38;5;241m.\u001b[39mappend(collect_df_from_config(qps, protocol))\n\u001b[1;32m----> 5\u001b[0m idle_config \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_idle_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m configs\u001b[38;5;241m.\u001b[39mappend(idle_config)\n",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m, in \u001b[0;36mcollect_idle_df\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m ROOT \u001b[38;5;241m+\u001b[39m folder_name_start\n\u001b[0;32m      4\u001b[0m folder_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m((f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(ROOT) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mstartswith(folder_name_start)), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m----> 6\u001b[0m logs_dfs \u001b[38;5;241m=\u001b[39m \u001b[43mimport_all_logs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mROOT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m measures_dfs \u001b[38;5;241m=\u001b[39m import_csv_files(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ROOT, folder_name))\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logs_dfs,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeasures\u001b[39m\u001b[38;5;124m\"\u001b[39m: measures_dfs,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQPS\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProtocol\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midle\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m }\n",
      "Cell \u001b[1;32mIn[19], line 42\u001b[0m, in \u001b[0;36mimport_all_logs\u001b[1;34m(folder_path)\u001b[0m\n\u001b[0;32m     40\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m\"\u001b[39m, log_file)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaptured\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m log_file:\n\u001b[1;32m---> 42\u001b[0m     df, date_offset \u001b[38;5;241m=\u001b[39m \u001b[43mimport_log_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     df \u001b[38;5;241m=\u001b[39m import_log_file(file_path, offset_seconds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3600\u001b[39m, offset_date\u001b[38;5;241m=\u001b[39mdate_offset)\n",
      "Cell \u001b[1;32mIn[19], line 22\u001b[0m, in \u001b[0;36mimport_log_file\u001b[1;34m(file_path, offset_seconds, offset_date)\u001b[0m\n\u001b[0;32m     20\u001b[0m log_df\u001b[38;5;241m.\u001b[39mdropna(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offset_date \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     date_offset \u001b[38;5;241m=\u001b[39m \u001b[43mlog_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatetime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdate()\n\u001b[0;32m     24\u001b[0m log_df \u001b[38;5;241m=\u001b[39m log_df\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msort_index()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offset_date:\n",
      "File \u001b[1;32mc:\\Fernand\\programmation\\dns-activity-monitoring\\.venv\\lib\\site-packages\\pandas\\core\\indexing.py:1192\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m   1191\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[1;32m-> 1192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Fernand\\programmation\\dns-activity-monitoring\\.venv\\lib\\site-packages\\pandas\\core\\indexing.py:1753\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1752\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[1;32m-> 1753\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1755\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32mc:\\Fernand\\programmation\\dns-activity-monitoring\\.venv\\lib\\site-packages\\pandas\\core\\indexing.py:1686\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1684\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[0;32m   1685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[1;32m-> 1686\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "configs = []\n",
    "for protocol in CONFIGURATIONS[\"Protocols\"]:\n",
    "    for qps in CONFIGURATIONS[\"QPS\"]:\n",
    "        configs.append(collect_df_from_config(qps, protocol))\n",
    "idle_config = collect_idle_df()\n",
    "configs.append(idle_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3c78e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'idle_20260217_084146'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# idle_config = collect_idle_df()\n",
    "folder_name_start = f\"idle_\"\n",
    "folder_name = next((f for f in os.listdir(ROOT) if f.startswith(folder_name_start)), None)\n",
    "folder_name\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
